{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n",
    "<b>Hackathon 04 - Clustering: What is it all about?</b> </font> <br><br><br>\n",
    "\n",
    "<font size=5  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br> \n",
    "Cécile Hautecoeur    (cecile.hautecoeur@uclouvain.be)<br> \n",
    "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br> \n",
    "Loïc Van Hoorebeeck  (loic.vanhoorebeeck@uclouvain.be)<br> \n",
    "<div style=\"text-align: right\"> Version 2020</div>\n",
    "\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n",
    "-  This assignment is due on the <b>21 December 2020 at noon</b>.\n",
    "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups,...) must be clearly indicated!</b>\n",
    "-  The notebook (with the \"ipynb\" extension) file and all other files that are necessary to run your code must be delivered on <b>Moodle</b>. Answer to the questions in this notebook in the markdown cells provided for that purpose. <br><br>\n",
    "\n",
    "\n",
    "<font size=5 color=#009999> <b>CONTEXT & NOTEBOOK STRUCTURE</b> </font> <br>\n",
    "    \n",
    "The objective of this hackathon is to develop an unsupervised text clustering tool. Given a large dataset containing documents (questions posted on a forum) relative to different topics, you should be able to group texts belonging to similar topics.<br><br> \n",
    "\n",
    "<img src=\"Imgs/text_clustering.png\" width = \"400\">\n",
    "\n",
    "Such text content is everywhere : emails, chat conversations, websites, and social media. Assigning categories to documents has many applications like spam filtering, email routing, sentiment analysis, etc. <br>\n",
    "Text can be an extremely rich source of information. However, extracting insights from it can be hard and time-consuming due to its unstructured nature. Natural Language Processing (NLP) is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. It has for objective to read, understand and make sense of the human languages in a manner that is valuable. Recently, Text classifiers with NLP have proven to be a great alternative to structure textual data in a fast, cost-effective, and scalable way.<br><br> \n",
    "\n",
    "This notebook is organized into three parts. Each of them assesses one fundamental step to solve our problem and provides one visualization tool to gain some understanding:\n",
    "* PART 1 - TEXT PREPROCESSING\n",
    "   - 1.1 - Import the data\n",
    "   - 1.2 - Text data preprocessing\n",
    "   - 1.3 - Preliminary visualization tool \n",
    "    <br><br>\n",
    "* PART 2 - THE FEATURE MATRIX \n",
    "   - 2.1 - Tf-idf model\n",
    "   - 2.2 - Visualization of the tf-idf model \n",
    "    <br><br>\n",
    "* PART 3 - IT'S TIME TO ... CLUSTER!\n",
    "   - 3.1 - Text clustering\n",
    "   - 3.2 - Results analysis \n",
    "   \n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 1 - TEXT PREPROCESSING</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>1.1 - IMPORT THE DATA</b> <br>\n",
    "\"Transfer Learning on Stack Exchange Tags\" dataset\n",
    "</font> <br> <br>\n",
    "\n",
    "In this __[dataset](https://www.kaggle.com/c/transfer-learning-on-stack-exchange-tags/notebooks)__, you are provided with question titles, contents, and tags for Stack Exchange sites on a variety of topics (biology, cooking, cryptography, diy, robotics, and travel). The content of each question is given as HTML. The tags are words that describe the topic of the question.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Consider only a part of the dataset</b>  <br>\n",
    "For computational and memory issues, we suggest you to work with a part of this data set (typically 3 topics). The code below allows you to select the topics (crypto, cooking, travel by default) you want to process and store corresponding data in a dictionnary. \n",
    "Moreover, for memory purpose, we will work on the title of the question excusively (the content of the question will not take part in the clustering process). Also, for the same reason, we consider the first 5000 questions of each topic. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1 : IMPORT THE DATASET\n",
    "[To do] You can modify the 3 topics that you will process (by default : crypto, cooking & travel)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# All topics considered in the dataset -> choose 3 of them by setting the dictionnary value to \"True\"\n",
    "expConsidered = {'biology': False, \n",
    "                 'robotics': False, \n",
    "                 'crypto': True, \n",
    "                 'diy': False, \n",
    "                 'cooking': True, \n",
    "                 'travel': True}\n",
    "\n",
    "# Construct our (reduced in size) dataset\n",
    "data = {}\n",
    "for thisLabel in expConsidered: \n",
    "    if expConsidered[thisLabel]:\n",
    "        filePath        = 'Data/' + thisLabel +'.csv'\n",
    "        data[thisLabel] = pd.read_csv(filePath, usecols = ['title'], nrows=5000)\n",
    "\n",
    "# Print some examples\n",
    "data['crypto'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.2 - TEXT DATA PREPROCESSING</b> <br>\n",
    "REMOVING UNNECESSARRY INFORMATION AND RETURN THE ROOTS OF THE WORDS\n",
    "</font> <br> <br>\n",
    "\n",
    "Not surprisingly, we cannot directly feed our text into our unsupervided clustering algorithms. We need to represent our data in an exploitable way for the clustering tools. Before developing the model, we need to perform an extensive preprocessing step over our dataset:\n",
    "\n",
    "1. **Data Cleaning** In order to avoid introducing bias in the word frequency analysis, we will remove unwanted (sequences of) charaters: \n",
    "    - Dealing with uppercase & lowercase letters;\n",
    "    - Removing punctuation;\n",
    "    - Apostrophe removal;\n",
    "    - Split attached words.<br>\n",
    "    \n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Regular Expressions </b>  <br>\n",
    "The last 3 steps have been implemented with regular expressions. As detailed in this __[article](https://towardsdatascience.com/regular-expressions-in-python-a212b1c73d7f)__ (\"Towards data science\" blog), Regular expressions <i>are a generalized way to match patterns with sequences of characters. They define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. “find and replace” like operations.</i><br>\n",
    "Do not hesistate to read more about regular expressions. These tools are extremely useful. \n",
    "</div> \n",
    "\n",
    "2. **Stopword Removal** Discarding too common words or words which are not going to be helpful in our analysis  (such as “the”, “a”, “an”, “in”).\n",
    "3. **Data Tokenization** Segregation of text into individual words i.e tokens.\n",
    "4. **Stemming** Combining different variants of words into a single parent word that conveys the same meaning. For example, \"playing\", \"plays\" and \"played\" are mapped to \"play\"; \"am\", \"are\", \"is\" are mapped to \"be\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2 : TEXT DATA PREROCESSING\n",
    "[To do] Read the code bellow to understand how the text data preprocessing can be implemented. Then, run the cell.\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map POS tag to first character lemmatize() accepts\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Stop words\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_title(table):\n",
    "    title = table.title\n",
    "    # Converting text to lowercase characters\n",
    "    title = title.apply(lambda x: x.lower())\n",
    "    # Removing any character which does not match to letter,digit or underscore\n",
    "    title = title.apply(lambda x: re.sub(r'^\\W+|\\W+$',' ',x))\n",
    "    # Removing space,newline,tab\n",
    "    title = title.apply(lambda x: re.sub(r'\\s',' ',x))\n",
    "    # Removing punctuation\n",
    "    title = title.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "    # Tokenizing and Stemming data + stop word removal\n",
    "    title = title.apply(lambda x: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_tokenize(x) if w not in stops] )\n",
    "    return(title)\n",
    "\n",
    "# Apply our preprocessing method and observe the resultFccr\n",
    "for df in data:\n",
    "    data[df].title = clean_title(data[df])\n",
    "\n",
    "data['crypto'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.3 - PRELIMINARY VISUALIZATION TOOL</b> <br>\n",
    "WORD CLOUD\n",
    "</font> <br> <br>\n",
    "\n",
    "Below, we propose a visualizing tool named \"word cloud\" allowing to summarize the content of multiple texts. A word cloud is an image made of words that together resemble a cloudy shape. The size of a word shows how important it is e.g. how often it appears in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3 : WORD CLOUD\n",
    "[To do] This is only an original tool to represent your data. Run the cell and observe the result.\n",
    "\"\"\"\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = ' '\n",
    "for x in data['crypto'].title:\n",
    "    for y in x:\n",
    "        text+=' ' + y\n",
    "        \n",
    "plt.figure(figsize=(12,15))\n",
    "wc = WordCloud(max_words=1000,random_state=1).generate(text)\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=7 color=#009999> <b>PART 2 - THE FEATURE MATRIX<b> </font> <br><br>\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> <b>2.1 - TF-IDF MODEL</b> <br>\n",
    "CONVERT TEXT SENTENCES INTO NUMERIC VECTORS\n",
    "</font> <br> <br>\n",
    "\n",
    "TF-IDF stands for “Term Frequency - Inverse Document Frequency”. TF-IDF is intended to reflect how relevant a term is in a given document. Intuitively, a word that occurs many times in a document should be relevant compared to very rare words. However, a word that appears too often in a document, but also along many other documents, may be irrelavant to characterize the document's topic. \n",
    "\n",
    "Based on this intuition, each word in the corpus (technical term that refers to all texts considered - here, the corpus is the processed 'data' dictionnary) will receive a weight measure computed by the product of two terms: \n",
    "1. **Normalized Term Frequency (tf)** given by: <br>\n",
    "    TF = (Number of repetitions of word in a document) / (# of words in a document)\n",
    "2. **Inverse Document Frequency (idf)** given by : <br>\n",
    "    IDF =Log[(# Number of documents) / (Number of documents containing the word)]\n",
    "    \n",
    "Based on this formula, we are able to construct an $m \\times n$ matrix where $m$ is the number of documents and $n$ the number of words in the corpus. The columns of the matrix represent then the feature space in which we will perform the clustering. \n",
    "For more detailed explanations, refer to this __[post](https://www.freecodecamp.org/news/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04/)__.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Obtain the tfidf matrix</b>  <br>\n",
    "Use the __[<samp>TfidfVectorizer</samp>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)__ methods to obtain the vectorized representation of data. <b>Read carefully the documentation and the example provided in the web page linked previously!!!</b> <br>\n",
    "A couple things to note about the parameters:\n",
    "<ol>\n",
    "   <li> **max_df**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    <li> **max_features**: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    <li>  **use_idf**: Enable inverse-document-frequency reweighting.\n",
    "</ol>\n",
    "</div> \n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.1] Creation of the data (also called tfidf) matrix </b>  <br>\n",
    "Use the <samp>TfidfVectorizer</samp> function from the <samp>sklearn.feature_extraction.text</samp> to create two tfidf matrices : \n",
    "<ol>\n",
    "   <li> <samp>XFull</samp> of dimension $m \\times n$ where $m$ is the number of documents and $n$ the vocabulary size. In this case $n$ in the number of different words in the corpus. \n",
    "   <li> <samp>X50</samp> of dimension $m \\times 50$ where $m$ is the number of documents. We consider here a vocabulary of 50 words which is somehow a light version of <samp>XFull</samp>.\n",
    "</ol>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4 : TFIDF MATRIX\n",
    "[To do] Construct the X50 and XFull matrices with the tf-idf model to replace our random features. \n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# X50 is a matrix with 15000 rows (3*5000 questions) and 50 features\n",
    "X50   = np.random.random((15000, 50))\n",
    "# The second dimension of XFull depends on the topics you have choosen (8000 is our random guess...)\n",
    "XFull = np.random.random((15000, 8000)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.2] All-zero representations? </b>  <br>\n",
    "For these two configurations (<samp>X50</samp> and <samp>XFull</samp>), count the number of documents whose encoded vector is the null vector [0,0,...0]. Why are they such vectors? How will them be treated by the clustering algorithm? (+/- 10 lines)\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 1.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>2.2 - VISUALIZATION OF THE TF-IDF MODEL</b> <br>\n",
    "REDUCE THE DIMENSIONALITY OF THE DATA FOR VISUALIZATION\n",
    "</font> <br> <br>\n",
    "\n",
    "The high dimensionality of the tfidf matrix (dimension of size $n$ for <samp>XFull</samp> or 50 for <samp>X50</samp>) makes data visualization hard. In order to gain some (partial) information about our data distribution, we propose to perform a 3-dimensional visualization of them. For this purpose, we need to reduce dimensionality of our feature space while keeping as much information as possible about the data. \n",
    "\n",
    "PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinate system to perform the visualization.\n",
    "\n",
    "\n",
    "![PCAUrl](https://miro.medium.com/max/400/1*ZXhPoYQIn-Y8mxoUpz5Ayw.gif \"PCA\")\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Low-rank approximation of the tf-idf matrix with Truncated SVD</b>  <br>\n",
    "One of the major drawbacks of the PCA decomposition is the need to strore the covariance matrix of the data. In our applicative context, this lead to heavy computations that are not optimized for sparse matrices. Instead, we suggest you to consider any decomposition which is particularly efficient on sparse data : the truncatedSVD. To obtain the decomposition, use the <samp>TruncatedSVD</samp> methods of the <samp>sklearn.decomposition</samp> library. \n",
    "</div> \n",
    "\n",
    "\n",
    "** Difference between PCA/SVD and Truncated SVD : linear algebra explanation **\n",
    "\n",
    "The SVD decomposition of a matrix $X \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$ is of the form :\n",
    "\n",
    "$$ X = U \\Sigma V^{\\text{T}} $$\n",
    "\n",
    "where $ U \\in \\mathbb{R}^{n \\times n}$, $ V \\in \\mathbb{R}^{m \\times m}$ and $ \\Sigma \\in \\mathbb{R}^{n \\times m}$ is a matrix with nonnegative entries on the diagonal and zeros off the diagonal. The columns of $U$ are called left singular vectors of $X$ and the columns of $V$ are right singular vectors. The diagonal elements of $\\Sigma$ are called singular values and the are ordered from largest to smallest. For matrix $X$ of rank $r$, there are exactly $r$ non zero singular values. \n",
    "\n",
    "Perhaps the most useful and defining property of the SVD is that it provides an optimal low-rank approximation to a  matrix $X$. In fact, the SVD provides a hierarchy of lowrank approximations, since a rank-$r$ approximation is obtained by keeping the leading $r$ singular values and vectors, and discarding the rest.\n",
    "\n",
    "The truncated SVD decomposition allows to compute a row-$k$ approximation of $X$ for a choosen $k$ parameter that is usually small and less than $r \\leq n $. The truncated SVD only approximates X:\n",
    "\n",
    "$$ X \\approx U_k^* \\Sigma_k^* V_k^{*\\text{T}} $$\n",
    "\n",
    "where $ U \\in \\mathbb{R}^{n \\times k}$, $ V \\in \\mathbb{R}^{k \\times m}$ and $ \\Sigma \\in \\mathbb{R}^{k \\times k}$. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2] Preliminary data visualization </b>  <br>\n",
    "Use the <samp> groupVisualization </samp> function provided below to perfom a 3-dimensional scatter plot of the data depending of the topic associated to the title's topic. Do this analysis for both <samp>XFull</samp> and <samp>X50</samp>.<br>\n",
    "Does the clustering problem seems to be difficult? Are the clusters separable? <br>\n",
    "Answer to these questions below (5-10 lines). \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°5 : TFIDF MATRIX VISUALIZATION\n",
    "[To do] Apply PCA on both X50 and XFull. Keep the 3 most relevant PCA components (or new features) to reduce\n",
    "        the dimensionality of the two data matrices. Then, plot the samples in the coordinate space of the 3 most \n",
    "        relevant components with the groupVisualization function. \n",
    "\"\"\"\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "3D Vizualisation of samples (color depends on the real topic = Ground Truth)\n",
    "INPUT : \n",
    "    - data: intitial data dictionnary\n",
    "    - X [Mx3]: data coordinates (in low dimensional space) after PCA\n",
    "    - (lessPoitns=False. If True, Display a random subset of points instead of the entire documents samples.\n",
    "    You can change it if you observe memory issues with your computer of very slow interractivity with the plot.)\n",
    "----------------------------------------------------------------------------------------- \"\"\" \n",
    "\n",
    "def groupVisualization(data, X, lessPoints=False):\n",
    "    colors      = iter(['gold','mediumaquamarine','midnightblue'])\n",
    "    all_3dplots = []\n",
    "    start       = 0\n",
    "    for label in data: \n",
    "        if lessPoints:     # Plot only a random subset of points\n",
    "            selectedPoints = np.random.choice(len(data[label]['title']), 1000) + start \n",
    "        else:              # Display all points \n",
    "            selectedPoints = range(start, start+ len(data[label]['title']) )\n",
    "        temp_cluster3d = go.Scatter3d(\n",
    "                                x = X[selectedPoints,0],\n",
    "                                y = X[selectedPoints,1],\n",
    "                                z = X[selectedPoints,2],\n",
    "                                mode = 'markers',\n",
    "                                opacity = 0.7,\n",
    "                                name = label,\n",
    "                                marker = dict(\n",
    "                                            size = 5,\n",
    "                                            color = next(colors)\n",
    "                                )\n",
    "        )\n",
    "        start += len(data[label]['title'])\n",
    "        all_3dplots.append(temp_cluster3d)\n",
    "    fig_3d = go.Figure(data = all_3dplots)\n",
    "    iplot(fig_3d)\n",
    "    \n",
    "    \n",
    "\n",
    "print('-'*75 + '\\n', \"Visualization of X50 \\n\", '-'*75 + '\\n')\n",
    "groupVisualization(data, X50[:,0:3]) # We can do better than X50[:,0:3], no?\n",
    "\n",
    "print('-'*75 + '\\n', \"Visualization of XFull \\n\", '-'*75 + '\\n')\n",
    "groupVisualization(data, XFull[:,0:3]) # We can do better than XFull[:,0:3], no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=7 color=#009999> <b>PART 3 - IT'S TIME TO ... CLUSTER!</b> </font> <br><br>\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> <b>3.1 - TEXT CLUSTERING</b> <br>\n",
    "THE K-MEANS ALGORITHM\n",
    "</font> <br> <br>\n",
    "\n",
    "Clustering can be defined as the task of grouping a set of objects in such a way that objects in the same group (called cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).\n",
    "\n",
    "\n",
    "In other words, this corresponds to finding, among all $K$-partitions $C_1 \\cup C_2 \\cup... \\cup C_K$, the one that minimizes the Sum of the Squared Errors (SSE)\n",
    "$$ \\text{min}_{C_1 \\cup C_2 \\cup... \\cup C_K} ~\\sum_{i=1}^{K} ~\\sum_{\\textbf{x} \\in C_i}~ dist(\\textbf{c}_i, \\textbf{x})^2 $$ where $c_i$ denotes the mean of each cluster $C_k$ (=centroids).\n",
    "\n",
    "![KMeansUrl](https://dashee87.github.io/images/kmeans.gif \"KMeans\")\n",
    "\n",
    "As previously mentioned, the goal of the method is to find K centroids as far away from each other and assign each object to the nearest centroid. For this purpose, the algorithm iterates between two steps until a stopping criteria is reached (i.e., data points are assigned to the same cluster, maximum number of iterations,...):\n",
    "1. **Data assignment step:**<br>\n",
    "    Each data point is assigned to its nearest centroid, (usually) based on the Euclidean distance.\n",
    "2. **Centroid update step:**<br>\n",
    "    Centroids are recomputed by taking the mean of all data points assigned to that centroid’s cluster. At this setp, what could happend if one centroid has no assigned data point? Find a way to handle properly such a situation. \n",
    "\n",
    "This process is depicted by the following pseudo-code.\n",
    "\n",
    "<img src=\"Imgs/KMeans.png\" width = \"650\">\n",
    "\n",
    "K-Means algorithm is guaranteed to converge to a result. However, the result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.1] Implement the K-Means algorithm </b>  <br>\n",
    "Based on the pseudo-code given above, implement your version of the  K-Means algorithm. Your algorihtm should work for both euclidean and cosine distance metrics (given by the <samp>euclidean_distances</samp> and <samp> cosine_similarity</samp> functions from the <samp>sklearn.metrics.pairwise</samp> library. <br>\n",
    "What are the pros and cons of this algorithm? (+/- 15 lines)<br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°6 : K-MEANS\n",
    "[To do] Implement the K-Means algorithm with the specifications listed below. \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "K-Means clustering method\n",
    "INPUT : \n",
    "    - X [MxN] : data to cluster\n",
    "    - K : number of clusters\n",
    "    - nb_epochs : maximum number of epochs of the algorithm. You should think about a proper way to stop the \n",
    "                  K-Means loop...\n",
    "    - (dist ='euclidean'. The distance metric used for the clustering. Your function should also support the \n",
    "        option dist ='cosine')\n",
    "OUTPUT :\n",
    "    - clusters [Mx] : list of assigned cluster to each data sample \n",
    "    - centroids [KxN] : location of each centroid\n",
    "----------------------------------------------------------------------------------------- \"\"\" \n",
    "\n",
    "def KMeans(X, K, nb_epochs, dist = 'euclidean'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Write the K-Means clustering function\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters  = np.random.random_integers(0, high=2, size=(X.shape[0],))\n",
    "    centroids = np.random.random((K,X.shape[0]))  \n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.2] Visualize the K-Means clustering result </b>  <br>\n",
    "Use the <samp>clustersVisualization</samp> function to visualize the clustering obtained both on <samp>X50</samp> and <samp>XFull</samp>.\n",
    "Do you observe diffrences between the euclidian and cosine distances? Provide an intuitive explanation/interpretation of those results. (+/- 10 lines)\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°6 : K-MEANS VISUALIZATION\n",
    "[To do] If your K-Means is well implemented, run this cell to produce the graphs. Feel free to modify the code \n",
    "        if needed. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "3D Vizualisation of cluster result\n",
    "INPUT : \n",
    "    - data : intitial data dictionnary\n",
    "    - X [Mx3] : data coordinates (in low dimensional space) after PCA\n",
    "    - clusters [Mx] : cluster assignation of each data point\n",
    "    - (lessPoitns=False. If True, Display a random subset of points instead of the entire documents samples.\n",
    "    You can change it if you observe memory issues with your computer of very slow interractivity with the plot.)\n",
    "----------------------------------------------------------------------------------------- \"\"\" \n",
    "\n",
    "def clustersVisualization(data, X, clusters, lessPoints=False): \n",
    "    colors    = iter(['gold','mediumaquamarine','midnightblue'])\n",
    "    all_plots = []\n",
    "    start     = 0\n",
    "    for idx,label in enumerate(data): \n",
    "        if lessPoints:     # Plot only a random subset of points\n",
    "            selectedPoints = np.random.choice(len(data[label]['title']), 1000) + start \n",
    "        else:              # Display all points \n",
    "            selectedPoints = range(start, start+ len(data[label]['title']) )\n",
    "        temp_cluster = go.Scatter3d(\n",
    "                                x = X[clusters == idx,0],\n",
    "                                y = X[clusters == idx,1],\n",
    "                                z = X[clusters == idx,2],\n",
    "                                mode = 'markers',\n",
    "                                opacity = 0.7,\n",
    "                                name = str(idx),\n",
    "                                marker = dict(\n",
    "                                            size = 5,\n",
    "                                            color = next(colors)\n",
    "                                )\n",
    "        )\n",
    "        all_plots.append(temp_cluster)\n",
    "    fig = go.Figure(data = all_plots)\n",
    "    iplot(fig)\n",
    "    \n",
    "    \n",
    "print('-'*75 + '\\n', \"Visualization of K-Means over X50 (euclidian distance) \\n\", '-'*75 + '\\n')\n",
    "clustersEuclidian50, centroidsEuclidian50 = KMeans(X50, 3, 1, dist = 'euclidean')\n",
    "clustersVisualization(data, X50[:,0:3], clustersEuclidian50)\n",
    "\n",
    "print('-'*75 + '\\n', \"Visualization of K-Means over X50 (cosine distance) \\n\", '-'*75 + '\\n')\n",
    "clustersCosine50, centroidsCosine50 = KMeans(X50, 3, 1, dist = 'cosine')\n",
    "clustersVisualization(data, X50[:,0:3], clustersCosine50)\n",
    "\n",
    "print('-'*75 + '\\n', \"Visualization of K-Means over XFull (euclidian distance) \\n\", '-'*75 + '\\n')\n",
    "clustersEuclidianFull, centroidsEuclidianFull = KMeans(XFull, 3, 1, dist = 'euclidean')\n",
    "clustersVisualization(data, XFull[:,0:3], clustersEuclidianFull)\n",
    "\n",
    "print('-'*75 + '\\n', \"Visualization of K-Means over XFull (cosine distance) \\n\", '-'*75 + '\\n')\n",
    "clustersCosineFull, centroidsCosineFull = KMeans(XFull, 3, 1, dist = 'cosine')\n",
    "clustersVisualization(data, XFull[:,0:3], clustersCosineFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>3.2 - RESULTS ANALYSIS</b> <br>\n",
    "SILHOUETTE PLOT AND CONFUSION MATRICES\n",
    "</font> <br> <br>\n",
    "\n",
    "In this section, we adress the difficult task of evaluating the performance of the clustering algorithm. We suggest you to work in 3 steps : \n",
    "\n",
    "<br> \n",
    "1. <b>Look at the result.</b> Before any quantitative study, you should look at your clustering in the low dimensional space. It should help you to get some insight. (We already made this step in the past cells). \n",
    "<br><br>    \n",
    "2. <b>Perform a silhouette analysis.</b> One way to assess the quality of the data partitionning is the use of the silhouette analysis. Each silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The silhouette coefficient is computed as follows:<br>\n",
    "     &emsp; 1. For each observation $i$, calculate the average dissimilarity $a_i$ between $i$ and all other points of the cluster to which $i$ belongs.<br>\n",
    "     &emsp; 2. For all other clusters $C$, to which $i$ does not belong, calculate the average dissimilarity $d(i,C)$ of $i$ to all observations of $C$. The smallest of these $d(i,C)$ is defined as $b_i=min_C d(i,C)$. The value of $b_i$ can be seen as the dissimilarity between $i$ and its \"neighbor\" cluster, i.e., the nearest one to which it does not belong.<br>\n",
    "     &emsp; 3. Finally the silhouette width of the observation $i$ is defined by the formula: $S_i = (b_i−a_i)/max(a_i,b_i)$.<br>\n",
    "    \n",
    "The coefficient varies between -1 and 1. A value close to 1 implies that the instance is close to its cluster is a part of the right cluster. Whereas, a value close to -1 means that the value is assigned to the wrong cluster.\n",
    "<br><br>\n",
    "3. <b>Compute the confusion matrices.</b> In the context where some labeled data are available for assigning clusters to known labels, we need to find which cluster corresponds to which label. For this purpose we have to find the label permutation that leads to the best [accuracy](https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/): \n",
    "    $$ \\text{accuracy}(y, \\hat{y}) = \\max_{\\text{perm} \\in P} \\frac{1}{n} \\sum_{i=0}^{n-1} 1(\\text{perm}(\\hat{y}_i) = y_i) $$\n",
    "where $x \\rightarrow 1(x)$ is the indicator function: $1(\\hat{y}_i = y_i) = 1$ if $\\hat{y}_i = y_i$ and 0 else. $P$ is the set of all permutations in $[1; K]$ with $K$ the number of clusters. Note that there are $K!$ premutations for which we will show the confusion matrix.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 4] Analyze results of your clustering method </b>  <br>\n",
    "Based on the 2 functions given below, estimate the quality of clustering obtained for the configurations/use cases used since the beginning:\n",
    "<ol>\n",
    "    <li> Use the K-Means algorithm on the <samp>X50</samp> data matrix with <samp>dist='euclidian'</samp> to retrieve the original 3 document clusters. \n",
    "    <li> Use the K-Means algorithm on the <samp>X50</samp> data matrix with <samp>dist='cosine'</samp> to retrieve the original 3 document clusters. \n",
    "    <li> Use the K-Means algorithm on the <samp>XFull</samp> data matrix with <samp>dist='euclidian'</samp> to retrieve the original 3 document clusters. \n",
    "    <li> Use the K-Means algorithm on the <samp>XFull</samp> data matrix with <samp>dist='cosine'</samp> to retrieve the original 3 document clusters. \n",
    "</ol>\n",
    "Comment your results (+/- 25 lines).\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°7 : K-MEANS RESULT ANALYSIS\n",
    "[To do] If your K-Means is well implemented, run this cell to produce the graphs. Feel free to modify the code \n",
    "        if needed. \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import permutations\n",
    "import seaborn as sns; sns.set()\n",
    "        \n",
    "\"\"\"--------------------------------------------------------------------------------------------------\n",
    "Draw silhouettes of clusters\n",
    "    - X [MxN]: data coordinates\n",
    "    - clusters [Mx]: cluster assignation of each data point\n",
    "--------------------------------------------------------------------------------------------------\"\"\"\n",
    "def silhouetteVisualization(X, clusters, metric='euclidean'):\n",
    "    colors = iter(['gold','mediumaquamarine','midnightblue'])\n",
    "    fig    = plt.figure(figsize=(6,5))\n",
    "    ax     = fig.add_subplot(111)\n",
    "\n",
    "    silhouette_avg           = silhouette_score(X, clusters, metric=metric)\n",
    "    sample_silhouette_values = silhouette_samples(X, clusters, metric=metric)\n",
    "    y_lower = 10\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    n_clusters = (np.unique(clusters)).shape[0]\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[clusters == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper        = y_lower + size_cluster_i\n",
    "\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                            0, ith_cluster_silhouette_values,\n",
    "                            facecolor=next(colors),edgecolor='k', \n",
    "                          alpha=0.7)\n",
    "    \n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    \n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "    \n",
    "    ax.set_title(\"Silhouette of predicted clusters.\")\n",
    "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Confusion Matrix resulting of the clustering\n",
    "INPUT : \n",
    "    - data : intitial data dictionnary\n",
    "    - clusters [Mx] : cluster assignation of each data point\n",
    "----------------------------------------------------------------------------------------- \"\"\" \n",
    "def confusionMatrixVisualization(data, clusters): \n",
    "    \n",
    "    target = []\n",
    "    for label in data: \n",
    "        target += [label]*len(data[label]['title'])\n",
    "    \n",
    "    originalLabels = list(set(target))\n",
    "    dic        = {}\n",
    "    for idx, label in enumerate(originalLabels): \n",
    "        dic[label] = idx\n",
    "    target_bin = [dic.get(n, n) for n in target]\n",
    "    \n",
    "    allPermutations = list(permutations(range(len(dic))))\n",
    "    WellClassified = []\n",
    "    \n",
    "    f, axes = plt.subplots(2, 3, figsize=(20,10))\n",
    "    rowPlot = 0\n",
    "    for idx, permutation in enumerate(allPermutations):\n",
    "        \n",
    "        newTarget = [permutation[x] for x in clusters]\n",
    "                \n",
    "        cm = confusion_matrix(target_bin, newTarget)\n",
    "        WellClassified.append(sum(cm[i,i] for i in range(cm.shape[0])))        \n",
    "        \n",
    "        ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[rowPlot, idx%3])\n",
    "        ax.set_title('Confusion Map ' + str(idx+1))\n",
    "        if idx%3 ==2 : \n",
    "            rowPlot += 1\n",
    "    print('The best classification (', str(np.max(WellClassified)), 'samples) is obtained with Confusion Map n°', \n",
    "          str( np.argmax(WellClassified)+1))\n",
    "    plt.show()\n",
    "\n",
    "print('-'*75 + '\\n', \"Result analysis of K-Means over X50 (euclidian distance) \\n\", '-'*75 + '\\n')\n",
    "silhouetteVisualization(X50, clustersEuclidian50, metric='cosine')\n",
    "confusionMatrixVisualization(data, clustersEuclidian50)\n",
    "\n",
    "print('-'*75 + '\\n', \"Result analysis of K-Means over X50 (cosine distance) \\n\", '-'*75 + '\\n')\n",
    "silhouetteVisualization(X50, clustersCosine50, metric='cosine')\n",
    "confusionMatrixVisualization(data, clustersCosine50)\n",
    "\n",
    "print('-'*75 + '\\n', \"Result analysis of K-Means over XFull (euclidian distance) \\n\", '-'*75 + '\\n')\n",
    "silhouetteVisualization(XFull, clustersEuclidianFull, metric='cosine')\n",
    "confusionMatrixVisualization(data, clustersEuclidianFull)\n",
    "\n",
    "print('-'*75 + '\\n', \"Result analysis of K-Means over XFull (cosine distance) \\n\", '-'*75 + '\\n')\n",
    "silhouetteVisualization(XFull, clustersCosineFull, metric='cosine')\n",
    "confusionMatrixVisualization(data, clustersCosineFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>3.3 - THE REAL WORLD IS SCARY</b> <br>\n",
    "FIND AN EFFICIENT WAY TO REDUCE THE DIMENSIONS OF XFULL WIHOUT LOOSING TOO MUCH INFORMATION...\n",
    "</font> <br> <br>\n",
    "\n",
    "At this stage, you should have noticed that processing the entire <samp>XFull</samp> data matrix is very slow in comparison to the <samp>X50</samp> data matrix. (Remember that we are working with only a small part of the data set: 5000 documents/topic, 3 topics, only the questions titles and not the answers, etc.). Could you image the time and ressources needed to process a new <samp>XReeeeallyBIG</samp> data matrix constructed as <samp>XFull</samp> but over the entire data set?\n",
    "\n",
    "Up to now we proposed to work with the <samp>X50</samp> data matrix as compromise. But... can we do better than <samp>X50</samp>? \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 5] Reduce the dimensions of <samp>XFull</samp> </b> <br>\n",
    "Compute a new document matrix named <samp>Xreduced50</samp> obtained by applying a truncated SVD with $k=50$ on the <samp>XFull</samp> matrix. Then, use the K-Means algorithm with $K=3$ and <samp>dist='cosine'</samp> to retrieve the original 3 document clusters. <br>\n",
    "Comment your results (+/- 10 lines).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°8 : REDUCE THE DIMENSIONS OF XFULL\n",
    "[To do] It is your turn to play to answer to this open question ;-) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert here your answer to Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
